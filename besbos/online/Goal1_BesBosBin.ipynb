{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEMABbP7pJ5"
      },
      "outputs": [],
      "source": [
        "#to see the contents of the cloud storage how the dataset it unzipped\n",
        "!ls -la /root/.keras/datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6iTKqUsXDCT"
      },
      "source": [
        "# Besilesomab and Bonescan Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORikQjzuBy4b"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# The simplest way is to use google drive because it is well connected together\n",
        "# This is a simple flow how to train something, methods and programming improvements were let out to be more understanble for NMPs\n",
        "\n",
        "#PATH to the dataset\n",
        "MAIN_PATH = \"/content/drive/GOOGLE_DRIVE_PATH\"\n",
        "IMG_SIZE = (900, 252)\n",
        "\n",
        "#dataset path with name\n",
        "DATASET = \"PATH_TO_ZIPPED_DATASET/DATASET_NAME_WITHOUT_ZIP\"\n",
        "\n",
        "#classes in the dataset\n",
        "# create a folder with subfolder bes and bos with scans and zip them in one file\n",
        "CLASS_NAMES = [\"bes\",\"bos\"]\n",
        "\n",
        "# path to save the logs from tensorboard, Colab deletes all the file after some time\n",
        "# so on you google drive is can persists for offline analysis\n",
        "LOG_FILE = \"PATH_GOOGLE_DRIVE_WHERE_TO_SAVE_LOGS\"\n",
        "\n",
        "\n",
        "#model save path for the checkpoint\n",
        "MODEL_PATH = \"/content/sample_data/models/save_at_{epoch}_bin.keras\"\n",
        "\n",
        "# this enables to extract from zip into folder\n",
        "archiveTrain = tf.keras.utils.get_file(origin=f\"file://{MAIN_PATH}{DATASET}.zip\", extract=True)\n",
        "dataDirTrain = pathlib.Path(archiveTrain+\"/NAME_OF_THE_FILE_WITHOUT_ZIP_SUFFIX\");\n",
        "\n",
        "img_height = 900\n",
        "img_width = 252\n",
        "batch_size = 110\n",
        "number_of_classes = len(CLASS_NAMES)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "#this creates a nice training and validation dataset, although there is a way to make it manually\n",
        "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(dataDirTrain,\n",
        "                                           labels='inferred',\n",
        "                                           class_names = CLASS_NAMES,\n",
        "                                           batch_size=batch_size,\n",
        "                                           validation_split=0.3,\n",
        "                                           subset=\"both\",\n",
        "                                           seed=400,\n",
        "                                           shuffle=True,\n",
        "                                           image_size=(900,252))\n",
        "\n",
        "# verification that the classes are in the dataset\n",
        "class_names = train_ds.class_names\n",
        "print(\"class_name:\",class_names)\n",
        "\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(\"image\",image_batch.shape)\n",
        "  print(\"label\", labels_batch.shape)\n",
        "  break\n",
        "\n",
        "# calculation of initial class_weights\n",
        "# total_number_of_samples/(number_of_classes*sample_class_count)\n",
        "def get_class_weights(data_dir,class_names,training_split):\n",
        "  total_class_count=0\n",
        "  #to get the full sample count divided by the training set\n",
        "  for clazz in class_names:\n",
        "    total_class_count += len(os.listdir(pathlib.Path(os.path.join(dataDirTrain,clazz))))*training_split\n",
        "  #add each initial class weight to a dictionary\n",
        "  index = 0\n",
        "  dict = {}\n",
        "  for clazz in class_names:\n",
        "    class_count = len(os.listdir(pathlib.Path(os.path.join(dataDirTrain,clazz))))*training_split\n",
        "    class_weight = total_class_count/(number_of_classes*class_count)\n",
        "    dict.update({index:class_weight})\n",
        "    index += 1\n",
        "    print(\"Initial weights:\", dict)\n",
        "  return dict\n",
        "\n",
        "#sample caount for the exponential decay rate\n",
        "SAMPLES = 0\n",
        "#printing of the train dataset tensors based on batch size and sample size calculation\n",
        "for x, y in train_ds:\n",
        "  SAMPLES += y.shape[0]\n",
        "print(\"number of samples\",SAMPLES)\n",
        "\n",
        "#plotting the training dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "# we take the firt batch from the training dataset -> to see what we are feeding into the CNN\n",
        "take_files = train_ds.take(1)\n",
        "for images, labels in take_files:\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# data augmentation, here the data are augmented for the CNN to use\n",
        "data_augmentation_layers = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomContrast(0.02), #decresed amount of augmentation\n",
        "    tf.keras.layers.RandomBrightness(0.02),\n",
        "    tf.keras.layers.RandomRotation((-0.05,0.05))\n",
        "\n",
        "])\n",
        "\n",
        "# data is normalized -> this improves the learning and also memory\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "# prefetching the data\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "image_batch, labels_batch = next(iter(train_ds))\n",
        "first_image = image_batch[0]\n",
        "\n",
        "#plotting the augmentet samples\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation_layers(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "# size of the kernel to go over the feature map\n",
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "\n",
        "def simple_cnn2():\n",
        "  inputs = tf.keras.Input(shape=(900, 252, 3))\n",
        "  #this means we add the layers to the model -> makes it more CPU/GPU consuming, but\n",
        "  # you do not need to preprocess the data for inference, e.g. in your app\n",
        "  x=data_augmentation_layers(inputs)\n",
        "  x=normalization_layer(x)\n",
        "  x= tf.keras.layers.Conv2D(32,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  #x= tf.keras.layers.BatchNormalization()(x)\n",
        "  #x= tf.keras.layers.Conv2D(32,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  x=tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
        "  x=tf.keras.layers.Conv2D(64,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  #x= tf.keras.layers.BatchNormalization()(x)\n",
        "  #x=tf.keras.layers.Conv2D(64,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  x=tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
        "  x=tf.keras.layers.Conv2D(128,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  #x=tf.keras.layers.Conv2D(128,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  x=tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
        "  x=tf.keras.layers.Conv2D(512,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  #x=tf.keras.layers.Conv2D(512,kernel_size,padding=\"same\",activation='relu')(x)\n",
        "  x=tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
        "  x=tf.keras.layers.Flatten()(x)\n",
        "  #x=tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "  #x=tf.keras.layers.Dropout(0.5)(x)\n",
        "  x=tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  x=tf.keras.layers.Dropout(0.5)(x) # this is for decreasing the overfitting, some neurons are dropped in the CNN, https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        "  x=tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  outputs=tf.keras.layers.Dense(1, activation=None)(x)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model = simple_cnn2()\n",
        "\n",
        "epochs = 60\n",
        "# exponential decay rate based on batch size, subsequent stops and epochs\n",
        "initial_learning_rate = 0.0003\n",
        "final_learning_rate = 0.00001\n",
        "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/epochs)\n",
        "steps_per_epoch = int(SAMPLES/batch_size)\n",
        "print(\"steps per epoch:\",steps_per_epoch)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "                initial_learning_rate=initial_learning_rate,\n",
        "                decay_steps=steps_per_epoch,\n",
        "                decay_rate=learning_rate_decay_factor,\n",
        "                staircase=True)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "              # switch between stable learning rate and scheduler\n",
        "              optimizer=tf.keras.optimizers.Adam(3e-4),\n",
        "              #optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[ #metrics\n",
        "                        tf.keras.metrics.AUC(name=\"auc\"),\n",
        "                        tf.keras.metrics.TruePositives(name='tp'),\n",
        "                        tf.keras.metrics.FalsePositives(name='fp'),\n",
        "                        tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "                        tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "                        tf.keras.metrics.Precision(name='precision'),\n",
        "                        tf.keras.metrics.Recall(name='recall'),\n",
        "                        keras.metrics.BinaryAccuracy(name=\"acuraccy\")],\n",
        "              run_eagerly=False\n",
        "              )\n",
        "# the training/fit process can be manipulated so weights can be adjusted etc this is the simplest use for the callbacks\n",
        "callbacks = [\n",
        "    # this is logging the training process\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=f\"/content/drive/MyDrive/Colab/{LOG_FILE}\",write_graph=False),\n",
        "    # this saves the best models\n",
        "    tf.keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor=\"val_loss\"),\n",
        "    # this stops when the model begins to overfit\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=5, start_from_epoch=35,restore_best_weights=True)\n",
        "]\n",
        "\n",
        "model.summary()\n",
        "#training function\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs,\n",
        "  # the callback defined above\n",
        "  callbacks=callbacks,\n",
        "  # initial weights to use for the learning phase\n",
        "  class_weight=get_class_weights(dataDirTrain,CLASS_NAMES,0.7)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#it is better to use Colabs cloud storage for saving best models and then save them to your Google Drive\n",
        "model = tf.keras.models.load_model(\"/content/sample_data/models/CHOOSE_THE_BEST_MODEL.keras\")\n",
        "model.summary()\n",
        "model.save('/content/drive/MyDrive/PATH_TO_YOUR_GOOGLE_DRIVE/MODEL_NAME.keras')"
      ],
      "metadata": {
        "id": "IGRPZcS7Wtfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXgBcLLC7QtE"
      },
      "outputs": [],
      "source": [
        "# this enables to see the log output for the training process\n",
        "%load_ext tensorboard\n",
        "#%reload_ext tensorboard #in case you have it already running\n",
        "%tensorboard --logdir 'PATH_TO_YOUR_GOOGLE_DRIVE_WITH_THE_LOGS'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}