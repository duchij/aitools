{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LP7S9LT4Hl_"
      },
      "outputs": [],
      "source": [
        "#remove data from dirs\n",
        "!rm -r /content/sample_data/epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB1xHJ1spl7N"
      },
      "outputs": [],
      "source": [
        "# how to check tensorflow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H9nWJx-zrM3"
      },
      "outputs": [],
      "source": [
        "# how to mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liVHg_1rA1_f"
      },
      "outputs": [],
      "source": [
        "#to see the loaded dataset\n",
        "!ls -la /root/.keras/datasets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X15lwaTqbRU"
      },
      "source": [
        "# Multiclass analysis Bes/Bos Posit/Negat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBVc2T9B5WL0"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "\n",
        "# This is a simple script adapated for testing how to train a dataset for multiclass image classification\n",
        "# Serves as companion to show nuclear medicine physicians how to train a AI model\n",
        "# Code can be uncommented to try it out different scenarios\n",
        "# Best used with google drive\n",
        "\n",
        "#initial configuration options\n",
        "MAIN_PATH = \"/content/drive/PATH_TO_GOOGLE_DRIVE\"\n",
        "# PATH where to store the log results\n",
        "LOG_PATH = \"/content/drive/PATH_TO_GOOGLE_DRIVE/LOG_FOLDER\"\n",
        "# PATH where to store the best models\n",
        "MODEL_PATH = \"/content/sample_data/models/save_at_{epoch}_w11.keras\"\n",
        "IMG_SIZE = (900, 252)\n",
        "\n",
        "DATASET = \"NAME_OF_FILE_WITHOUT_ZIP_SUFFIX\"\n",
        "# loading of the zip file and uncompressing it so the images can be loaded into dataset\n",
        "archiveTrain = tf.keras.utils.get_file(origin=f\"file://{MAIN_PATH}{DATASET}.zip\", extract=True)\n",
        "dataDirTrain = pathlib.Path(os.path.join(pathlib.Path(archiveTrain).with_suffix('.zip'), DATASET))\n",
        "\n",
        "# class name division\n",
        "CLASS_NAMES = ['besnegat','besposit','bosnegat','bosposit']\n",
        "\n",
        "img_height = 900\n",
        "img_width = 252\n",
        "# this defines the number of steps in which a epoch will be learned\n",
        "batch_size = 100\n",
        "number_of_classes = 4\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# loading the data from the zip file into the dataset\n",
        "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(dataDirTrain,\n",
        "                                           labels='inferred',\n",
        "                                           class_names=CLASS_NAMES,\n",
        "                                           batch_size=batch_size,\n",
        "                                           #producing train validation split 0.3 means 70% for training 30% for validation\n",
        "                                           validation_split=0.3,\n",
        "                                           subset=\"both\",\n",
        "                                           seed=400,\n",
        "                                           shuffle=True,\n",
        "                                           image_size=IMG_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "#loading of a pretrained model for use\n",
        "baseModel = tf.keras.applications.Xception(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(900,252,3),\n",
        ")\n",
        "# freezing of all the layers in the model\n",
        "baseModel.trainable = False\n",
        "\n",
        "# calculation of initial class_weights\n",
        "# total_number_of_samples/(number_of_classes*sample_class_count)\n",
        "def get_class_weights(data_dir,class_names,training_split):\n",
        "  total_class_count=0\n",
        "  #to get the full sample count divided by the training set\n",
        "  for clazz in class_names:\n",
        "    total_class_count += len(os.listdir(pathlib.Path(os.path.join(dataDirTrain,clazz))))*training_split\n",
        "  #add each initial class weight to a dictionary\n",
        "  index = 0\n",
        "  dict = {}\n",
        "  for clazz in class_names:\n",
        "    class_count = len(os.listdir(pathlib.Path(os.path.join(dataDirTrain,clazz))))*training_split\n",
        "    class_weight = total_class_count/(number_of_classes*class_count)\n",
        "    dict.update({index:class_weight})\n",
        "    index += 1\n",
        "  return dict\n",
        "\n",
        "#sample caount for the exponential decay rate\n",
        "SAMPLES = 0\n",
        "#printing of the train dataset tensors based on batch size and sample size calculation\n",
        "for x, y in train_ds:\n",
        "  print(x.shape, y.shape)\n",
        "  SAMPLES += y.shape[0]\n",
        "class_names = train_ds.class_names\n",
        "print(\"Class names:\",class_names)\n",
        "print(\"number of samples\",SAMPLES)\n",
        "\n",
        "# print out the shape of the images and labels\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(\"image\",image_batch.shape)\n",
        "  print(\"label\", labels_batch.shape)\n",
        "  break\n",
        "\n",
        "# augmentation layers\n",
        "data_augmentation_layers = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "  #changing these parameters may affect the final result\n",
        "  tf.keras.layers.RandomRotation((-0.01,0.01)),\n",
        "  tf.keras.layers.RandomContrast(0.02),\n",
        "  tf.keras.layers.RandomBrightness(0.02),\n",
        "\n",
        "])\n",
        "# adding the augmentation layers to the training dataset\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation_layers(x), y))\n",
        "\n",
        "with_augmented_samples = 0\n",
        "for x,y in train_ds:\n",
        "  with_augmented_samples += y.shape[0]\n",
        "print(\"samples + augmented samples\",with_augmented_samples)\n",
        "\n",
        "#obtaining images and labels from the train dataset to be able to show them\n",
        "#this is just for visualisation to see what we train\n",
        "image_batch, labels_batch = next(iter(train_ds))\n",
        "plt.figure(figsize=(10, 10))\n",
        "take_files = train_ds.take(1)\n",
        "for images, labels in take_files:\n",
        "  print(len(labels))\n",
        "  ha = labels.numpy().astype(\"uint8\")\n",
        "  print(\"binary\",ha.ndim)\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    #plt.title(class_names[labels.numpy().astype(\"uint8\")[0][0]])\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "# this based on the loss function so basically [1,0,0,0] means besnegat\n",
        "def one_hot_encode(image, label):\n",
        "  # Assuming your labels are integers from 0 to 3\n",
        "  return image, tf.one_hot(label, number_of_classes)\n",
        "\n",
        "train_ds = train_ds.map(one_hot_encode)\n",
        "val_ds = val_ds.map(one_hot_encode)\n",
        "\n",
        "#caching of the data is performed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "def tModel():\n",
        "  inputs = tf.keras.Input(shape=(900, 252, 3))\n",
        "  #preprocessing the data so the images are in that format the model awaits\n",
        "  x = tf.keras.applications.xception.preprocess_input(inputs)\n",
        "  x = baseModel(x, training=False)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  # here can be different Fuly connected layers activated also with regularization\n",
        "  # just for testing\n",
        "  #x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "  #x = tf.keras.layers.Dropout(0.3)(x)\n",
        "  #x= tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "  #x = tf.keras.layers.Dropout(0.3)(x)  # Regularize with dropout\n",
        "  x= tf.keras.layers.Dense(256,activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # Regularize with dropout\n",
        "  #x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # Regularize with dropout\n",
        "  #x = tf.keras.layers.BatchNormalization()(x) #\n",
        "  x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # Regularize with dropout\n",
        "  #x = tf.keras.layers.BatchNormalization()(x)\n",
        "  # this is the final layer which classifies the image -> without any activation\n",
        "  # we use then the distribution function in the verification script -> we get a \"raw\" tensor output\n",
        "  outputs = tf.keras.layers.Dense(number_of_classes,name=\"outputs\", activation=None)(x)\n",
        "  return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = tModel() #creation of the model structure\n",
        "\n",
        "start_lr = 0.0001\n",
        "min_lr = 9.98e-5\n",
        "max_lr = 0.001\n",
        "rampup_epochs = 50\n",
        "sustain_epochs = 15\n",
        "exp_decay = 0.96\n",
        "#different types of learning schedulers -> for trying out\n",
        "def my_learner(epoch):\n",
        "  def lr_schedule(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "    if epoch < rampup_epochs:\n",
        "      learning_rate = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "    elif epoch < rampup_epochs + sustain_epochs:\n",
        "      learning_rate = max_lr\n",
        "    else:\n",
        "      learning_rate = (min_lr+max_lr) * math.exp(-exp_decay*epoch)\n",
        "    return learning_rate\n",
        "  return lr_schedule(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "\n",
        "def schedule_exp(epoch):\n",
        "  def lr(epoch, start_lr, exp_decay):\n",
        "     return start_lr * math.exp(-exp_decay*epoch)\n",
        "  return lr(epoch, start_lr, exp_decay)\n",
        "\n",
        "def schedule_one_cycle(epoch):\n",
        "  def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "    if epoch < rampup_epochs:\n",
        "      lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "    elif epoch < rampup_epochs + sustain_epochs:\n",
        "      lr = max_lr\n",
        "    else:\n",
        "      lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "    return lr\n",
        "  return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "\n",
        "\n",
        "epochs = 200\n",
        "initial_learning_rate = 0.0003\n",
        "final_learning_rate = 0.000001\n",
        "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/epochs)\n",
        "steps_per_epoch = int(SAMPLES/batch_size)\n",
        "print(\"steps per epoch:\",steps_per_epoch)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "                initial_learning_rate=initial_learning_rate,\n",
        "                decay_steps=steps_per_epoch,\n",
        "                decay_rate=learning_rate_decay_factor,\n",
        "                staircase=True)\n",
        "\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(my_learner, verbose=True)\n",
        "\n",
        "model.compile(\n",
        "    # if you use a learning rate scheduler callback this is without the learning parameter\n",
        "    #optimizer=tf.keras.optimizers.Adam(),\n",
        "    # this is learning rate scheduler added to the optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    # this is a constant learning rate\n",
        "    #optimizer=tf.keras.optimizers.Adam(learning_rate=9.98e-5),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    # here are all available metrices to use\n",
        "    metrics=[tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "           tf.keras.metrics.TruePositives(name='tp'),\n",
        "           tf.keras.metrics.FalsePositives(name='fp'),\n",
        "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "           tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "           #tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
        "           tf.keras.metrics.Precision(name='precision'),\n",
        "           tf.keras.metrics.Recall(name='recall'),\n",
        "           tf.keras.metrics.CategoricalAccuracy(name='accuracy')],\n",
        "    run_eagerly=False,\n",
        ")\n",
        "# model structure\n",
        "model.summary()\n",
        "\n",
        "#callback used during the traning to adapt or log the training process\n",
        "callbacks = [\n",
        "    # this enables to see the entire learning process in a nice UI\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=f\"{LOG_PATH}\",write_graph=False),\n",
        "    # saves the best models -> google drive can be used\n",
        "    # monitor -> what si monitored for the EarlyStoppping\n",
        "    tf.keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor=\"val_loss\"),\n",
        "    #here is the learning scheduler if this is used the optimizer start without learning parameter\n",
        "    #lr_callback\n",
        "    #this can be enabled -> the training will stop when the model begins to overfit\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=5, start_from_epoch=35,restore_best_weights=True)\n",
        "]\n",
        "\n",
        "#training the model\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs,\n",
        "  callbacks=callbacks,\n",
        "  # calculation of initial weights in case of inbalanced dataset, 0.7 is the training data split\n",
        "  class_weight=get_class_weights(dataDirTrain,CLASS_NAMES,0.7)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF5iWmrM9Vd2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model(\"/content/sample_data/epochs/MODEL_NAME.keras\")\n",
        "model.summary()\n",
        "model.save('/content/drive/MyDrive/GOOGLE_DRIVE_PATH/NAME_OF_THE_MODEL.keras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8CrotYxsAUs"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBLGUkIG_wCD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKmgQT_ZrM4-"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "#%reload_ext tensorboard\n",
        "#!kill 19207\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Colab/posnegat/logs11f_w'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}